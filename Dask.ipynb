{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c1f6336-72e7-4ded-b5c0-3309e107ad29",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"YourAppName\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e48b0362-0adc-40c4-8906-b9e8fa56deab",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark.databricks.io.cache.enabled is false\n"
     ]
    }
   ],
   "source": [
    "spark.conf.set(\"spark.databricks.io.cache.enabled\", \"false\")\n",
    "print(\"spark.databricks.io.cache.enabled is %s\" % spark.conf.get(\"spark.databricks.io.cache.enabled\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8cc2bc45-0e2b-4f56-8d59-d30feae350cf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python interpreter will be restarted.\nCollecting dask[complete]\n  Downloading dask-2024.5.2-py3-none-any.whl (1.2 MB)\nRequirement already satisfied: packaging>=20.0 in /databricks/python3/lib/python3.9/site-packages (from dask[complete]) (21.3)\nCollecting toolz>=0.10.0\n  Downloading toolz-0.12.1-py3-none-any.whl (56 kB)\nCollecting importlib-metadata>=4.13.0\n  Downloading importlib_metadata-7.1.0-py3-none-any.whl (24 kB)\nCollecting click>=8.1\n  Downloading click-8.1.7-py3-none-any.whl (97 kB)\nCollecting fsspec>=2021.09.0\n  Downloading fsspec-2024.5.0-py3-none-any.whl (316 kB)\nCollecting partd>=1.2.0\n  Downloading partd-1.4.2-py3-none-any.whl (18 kB)\nCollecting cloudpickle>=1.5.0\n  Downloading cloudpickle-3.0.0-py3-none-any.whl (20 kB)\nCollecting pyyaml>=5.3.1\n  Downloading PyYAML-6.0.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (738 kB)\nRequirement already satisfied: pyarrow>=7.0 in /databricks/python3/lib/python3.9/site-packages (from dask[complete]) (7.0.0)\nRequirement already satisfied: pyarrow-hotfix in /databricks/python3/lib/python3.9/site-packages (from dask[complete]) (0.5)\nCollecting lz4>=4.3.2\n  Downloading lz4-4.3.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\nCollecting zipp>=0.5\n  Downloading zipp-3.19.1-py3-none-any.whl (9.0 kB)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /databricks/python3/lib/python3.9/site-packages (from packaging>=20.0->dask[complete]) (3.0.4)\nCollecting locket\n  Downloading locket-1.0.0-py2.py3-none-any.whl (4.4 kB)\nRequirement already satisfied: numpy>=1.16.6 in /databricks/python3/lib/python3.9/site-packages (from pyarrow>=7.0->dask[complete]) (1.21.5)\nCollecting distributed==2024.5.2\n  Downloading distributed-2024.5.2-py3-none-any.whl (1.0 MB)\nCollecting dask-expr<1.2,>=1.1\n  Downloading dask_expr-1.1.2-py3-none-any.whl (205 kB)\nRequirement already satisfied: pandas>=1.3 in /databricks/python3/lib/python3.9/site-packages (from dask[complete]) (1.4.2)\nCollecting bokeh>=2.4.2\n  Downloading bokeh-3.4.1-py3-none-any.whl (7.0 MB)\nRequirement already satisfied: jinja2>=2.10.3 in /databricks/python3/lib/python3.9/site-packages (from dask[complete]) (2.11.3)\nCollecting msgpack>=1.0.0\n  Downloading msgpack-1.0.8-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (385 kB)\nCollecting tblib>=1.6.0\n  Downloading tblib-3.0.0-py3-none-any.whl (12 kB)\nRequirement already satisfied: tornado>=6.0.4 in /databricks/python3/lib/python3.9/site-packages (from distributed==2024.5.2->dask[complete]) (6.1)\nRequirement already satisfied: urllib3>=1.24.3 in /databricks/python3/lib/python3.9/site-packages (from distributed==2024.5.2->dask[complete]) (1.26.9)\nCollecting zict>=3.0.0\n  Downloading zict-3.0.0-py2.py3-none-any.whl (43 kB)\nCollecting sortedcontainers>=2.0.5\n  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\nRequirement already satisfied: psutil>=5.7.2 in /databricks/python3/lib/python3.9/site-packages (from distributed==2024.5.2->dask[complete]) (5.8.0)\nCollecting contourpy>=1.2\n  Downloading contourpy-1.2.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (304 kB)\nCollecting xyzservices>=2021.09.1\n  Downloading xyzservices-2024.4.0-py3-none-any.whl (81 kB)\nCollecting tornado>=6.0.4\n  Downloading tornado-6.4-cp38-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (435 kB)\nRequirement already satisfied: pillow>=7.1.0 in /databricks/python3/lib/python3.9/site-packages (from bokeh>=2.4.2->dask[complete]) (9.0.1)\nCollecting pandas>=1.3\n  Downloading pandas-2.2.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\nRequirement already satisfied: MarkupSafe>=0.23 in /databricks/python3/lib/python3.9/site-packages (from jinja2>=2.10.3->dask[complete]) (2.0.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /databricks/python3/lib/python3.9/site-packages (from pandas>=1.3->dask[complete]) (2.8.2)\nCollecting tzdata>=2022.7\n  Downloading tzdata-2024.1-py2.py3-none-any.whl (345 kB)\nRequirement already satisfied: pytz>=2020.1 in /databricks/python3/lib/python3.9/site-packages (from pandas>=1.3->dask[complete]) (2021.3)\nCollecting numpy>=1.16.6\n  Downloading numpy-1.26.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\nRequirement already satisfied: six>=1.5 in /databricks/python3/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas>=1.3->dask[complete]) (1.16.0)\nInstalling collected packages: zipp, toolz, locket, tzdata, pyyaml, partd, numpy, importlib-metadata, fsspec, cloudpickle, click, zict, xyzservices, tornado, tblib, sortedcontainers, pandas, msgpack, dask, contourpy, distributed, dask-expr, bokeh, lz4\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.21.5\n    Not uninstalling numpy at /databricks/python3/lib/python3.9/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-e5fa1c31-e351-4056-a9dc-f3f4a9c54411\n    Can't uninstall 'numpy'. No files were found to uninstall.\n  Attempting uninstall: click\n    Found existing installation: click 8.0.4\n    Not uninstalling click at /databricks/python3/lib/python3.9/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-e5fa1c31-e351-4056-a9dc-f3f4a9c54411\n    Can't uninstall 'click'. No files were found to uninstall.\n  Attempting uninstall: tornado\n    Found existing installation: tornado 6.1\n    Not uninstalling tornado at /databricks/python3/lib/python3.9/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-e5fa1c31-e351-4056-a9dc-f3f4a9c54411\n    Can't uninstall 'tornado'. No files were found to uninstall.\n  Attempting uninstall: pandas\n    Found existing installation: pandas 1.4.2\n    Not uninstalling pandas at /databricks/python3/lib/python3.9/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-e5fa1c31-e351-4056-a9dc-f3f4a9c54411\n    Can't uninstall 'pandas'. No files were found to uninstall.\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nscipy 1.7.3 requires numpy<1.23.0,>=1.16.5, but you have numpy 1.26.4 which is incompatible.\nSuccessfully installed bokeh-3.4.1 click-8.1.7 cloudpickle-3.0.0 contourpy-1.2.1 dask-2024.5.2 dask-expr-1.1.2 distributed-2024.5.2 fsspec-2024.5.0 importlib-metadata-7.1.0 locket-1.0.0 lz4-4.3.3 msgpack-1.0.8 numpy-1.26.4 pandas-2.2.2 partd-1.4.2 pyyaml-6.0.1 sortedcontainers-2.4.0 tblib-3.0.0 toolz-0.12.1 tornado-6.4 tzdata-2024.1 xyzservices-2024.4.0 zict-3.0.0 zipp-3.19.1\nPython interpreter will be restarted.\n"
     ]
    }
   ],
   "source": [
    "%pip install dask[complete]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58d0cf45-85c4-4641-a7ee-87eb248ab335",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python interpreter will be restarted.\nCollecting pyarrow==10.0.1\n  Downloading pyarrow-10.0.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35.9 MB)\nRequirement already satisfied: numpy>=1.16.6 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-e5fa1c31-e351-4056-a9dc-f3f4a9c54411/lib/python3.9/site-packages (from pyarrow==10.0.1) (1.26.4)\nInstalling collected packages: pyarrow\n  Attempting uninstall: pyarrow\n    Found existing installation: pyarrow 7.0.0\n    Not uninstalling pyarrow at /databricks/python3/lib/python3.9/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-e5fa1c31-e351-4056-a9dc-f3f4a9c54411\n    Can't uninstall 'pyarrow'. No files were found to uninstall.\nSuccessfully installed pyarrow-10.0.1\nPython interpreter will be restarted.\n"
     ]
    }
   ],
   "source": [
    "%pip install pyarrow==10.0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ee5d885-49de-429c-a6b1-d3f3b9b760c5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas version: 2.2.2\nnumpy version: 1.26.4\ndask version: 2024.5.2\npyarrow version: 10.0.1\npyspark version: 3.3.2.dev0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import databricks.koalas as ks\n",
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client, LocalCluster\n",
    "\n",
    "print('pandas version: %s' % pd.__version__)\n",
    "\n",
    "print('numpy version: %s' % np.__version__)\n",
    "\n",
    "#print('koalas version: %s' % ks.__version__)\n",
    "\n",
    "import dask\n",
    "print('dask version: %s' % dask.__version__)\n",
    "\n",
    "import pyarrow\n",
    "print('pyarrow version: %s' % pyarrow.__version__)\n",
    "\n",
    "import pyspark\n",
    "print('pyspark version: %s' % pyspark.__version__)\n",
    "\n",
    "\n",
    "import time\n",
    "\n",
    "client = Client()\n",
    "\n",
    "def benchmark(f, df, benchmarks, name, **kwargs):\n",
    "    \"\"\"Benchmark the given function against the given DataFrame.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    f: function to benchmark\n",
    "    df: data frame\n",
    "    benchmarks: container for benchmark results\n",
    "    name: task name\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Duration (in seconds) of the given operation\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    ret = f(df, **kwargs)\n",
    "    benchmarks['duration'].append(time.time() - start_time)\n",
    "    benchmarks['task'].append(name)\n",
    "    print(f\"{name} took: {benchmarks['duration'][-1]} seconds\")\n",
    "    return benchmarks['duration'][-1]\n",
    "\n",
    "def get_results(benchmarks):\n",
    "    \"\"\"Return a pandas DataFrame containing benchmark results.\"\"\"\n",
    "    return pd.DataFrame.from_dict(benchmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "067b3f14-3929-486e-b530-71fdc3fdd498",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "filenames = [f\"/FileStore/tables/yellow_tripdata_2023_0{i}.parquet\" for i in range(1, 6)]\n",
    "\n",
    "dfs = []\n",
    "for filename in filenames:\n",
    "    # df = pd.read_parquet(filename)\n",
    "    df = spark.read.format('parquet').options(header='true').load(filename).toPandas()\n",
    "\n",
    "    if 'airport_fee' in df.columns:\n",
    "        df.rename(columns={'airport_fee': 'Airport_fee'}, inplace=True)\n",
    "    df_dask = dd.from_pandas(df, npartitions=3)\n",
    "\n",
    "    dfs.append(df_dask)\n",
    "\n",
    "# pandas_data = pd.concat(dfs, ignore_index=True)\n",
    "dask_data = dd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "057600a4-73e6-40ab-aeeb-957d749b5d28",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[3]: 16186386"
     ]
    }
   ],
   "source": [
    "len(dask_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1325478d-0245-42ae-a5a1-214e81c01cd5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "dask_benchmarks = {\n",
    "    'duration': [],  # in seconds\n",
    "    'task': [],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54d64206-9e51-4bad-b2c0-d1b26ae8540b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def read_file_parquet(df=None):\n",
    "    return dd.read_parquet(\"/FileStore/tables/yellow_tripdata_2023_01.parquet\")\n",
    "  \n",
    "def count(df=None):\n",
    "    return len(df)\n",
    "\n",
    "def count_index_length(df=None):\n",
    "    return len(df.index)\n",
    "\n",
    "def mean(df):\n",
    "    return df.fare_amount.mean().compute()\n",
    "\n",
    "def standard_deviation(df):\n",
    "    return df.fare_amount.std().compute()\n",
    "\n",
    "def mean_of_sum(df):\n",
    "    return (df.fare_amount + df.tip_amount).mean().compute()\n",
    "\n",
    "def sum_columns(df):\n",
    "    return (df.fare_amount + df.tip_amount).compute()\n",
    "\n",
    "def mean_of_product(df):\n",
    "    return (df.fare_amount * df.tip_amount).mean().compute()\n",
    "\n",
    "def product_columns(df):\n",
    "    return (df.fare_amount * df.tip_amount).compute()\n",
    "  \n",
    "def value_counts(df):\n",
    "    return df.fare_amount.value_counts().compute()\n",
    "  \n",
    "def mean_of_complicated_arithmetic_operation(df):\n",
    "    theta_1 = df.start_lon\n",
    "    phi_1 = df.start_lat\n",
    "    theta_2 = df.end_lon\n",
    "    phi_2 = df.end_lat\n",
    "    temp = (np.sin((theta_2-theta_1)/2*np.pi/180)**2\n",
    "           + np.cos(theta_1*np.pi/180)*np.cos(theta_2*np.pi/180) * np.sin((phi_2-phi_1)/2*np.pi/180)**2)\n",
    "    ret = 2 * np.arctan2(np.sqrt(temp), np.sqrt(1-temp))\n",
    "    return ret.mean().compute()\n",
    "  \n",
    "def complicated_arithmetic_operation(df):\n",
    "    theta_1 = df.start_lon\n",
    "    phi_1 = df.start_lat\n",
    "    theta_2 = df.end_lon\n",
    "    phi_2 = df.end_lat\n",
    "    temp = (np.sin((theta_2-theta_1)/2*np.pi/180)**2\n",
    "           + np.cos(theta_1*np.pi/180)*np.cos(theta_2*np.pi/180) * np.sin((phi_2-phi_1)/2*np.pi/180)**2)\n",
    "    ret = 2 * np.arctan2(np.sqrt(temp), np.sqrt(1-temp))\n",
    "    return ret.compute()\n",
    "  \n",
    "def groupby_statistics(df):\n",
    "    return df.groupby(by='passenger_count').agg(\n",
    "      {\n",
    "        'fare_amount': ['mean', 'std'], \n",
    "        'tip_amount': ['mean', 'std']\n",
    "      }\n",
    "    ).compute()\n",
    "# other = groupby_statistics(dask_data)\n",
    "# other.columns = pd.Index([e[0]+'_' + e[1] for e in other.columns.tolist()])\n",
    "\n",
    "def join_count(df, other):\n",
    "    return len(dd.merge(df, other, left_index=True, right_index=True))\n",
    "\n",
    "def join_data(df, other):\n",
    "    return dd.merge(df, other, left_index=True, right_index=True).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eef5099b-6216-4f0e-9963-43e2df5c0808",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count took: 0.058249711990356445 seconds\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local_disk0/.ephemeral_nfs/envs/pythonEnv-e5fa1c31-e351-4056-a9dc-f3f4a9c54411/lib/python3.9/site-packages/distributed/client.py:3164: UserWarning: Sending large graph of size 2.10 GiB.\nThis may cause some slowdown.\nConsider scattering data ahead of time and using futures.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count index length took: 36.14887094497681 seconds\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local_disk0/.ephemeral_nfs/envs/pythonEnv-e5fa1c31-e351-4056-a9dc-f3f4a9c54411/lib/python3.9/site-packages/distributed/client.py:3164: UserWarning: Sending large graph of size 123.50 MiB.\nThis may cause some slowdown.\nConsider scattering data ahead of time and using futures.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean took: 2.394890069961548 seconds\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local_disk0/.ephemeral_nfs/envs/pythonEnv-e5fa1c31-e351-4056-a9dc-f3f4a9c54411/lib/python3.9/site-packages/distributed/client.py:3164: UserWarning: Sending large graph of size 123.50 MiB.\nThis may cause some slowdown.\nConsider scattering data ahead of time and using futures.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "standard deviation took: 1.5536243915557861 seconds\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local_disk0/.ephemeral_nfs/envs/pythonEnv-e5fa1c31-e351-4056-a9dc-f3f4a9c54411/lib/python3.9/site-packages/distributed/client.py:3164: UserWarning: Sending large graph of size 246.99 MiB.\nThis may cause some slowdown.\nConsider scattering data ahead of time and using futures.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean of columns addition took: 2.125427722930908 seconds\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local_disk0/.ephemeral_nfs/envs/pythonEnv-e5fa1c31-e351-4056-a9dc-f3f4a9c54411/lib/python3.9/site-packages/distributed/client.py:3164: UserWarning: Sending large graph of size 246.99 MiB.\nThis may cause some slowdown.\nConsider scattering data ahead of time and using futures.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "addition of columns took: 3.7850420475006104 seconds\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local_disk0/.ephemeral_nfs/envs/pythonEnv-e5fa1c31-e351-4056-a9dc-f3f4a9c54411/lib/python3.9/site-packages/distributed/client.py:3164: UserWarning: Sending large graph of size 246.99 MiB.\nThis may cause some slowdown.\nConsider scattering data ahead of time and using futures.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean of columns multiplication took: 1.7951135635375977 seconds\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local_disk0/.ephemeral_nfs/envs/pythonEnv-e5fa1c31-e351-4056-a9dc-f3f4a9c54411/lib/python3.9/site-packages/distributed/client.py:3164: UserWarning: Sending large graph of size 246.99 MiB.\nThis may cause some slowdown.\nConsider scattering data ahead of time and using futures.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multiplication of columns took: 3.9497592449188232 seconds\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local_disk0/.ephemeral_nfs/envs/pythonEnv-e5fa1c31-e351-4056-a9dc-f3f4a9c54411/lib/python3.9/site-packages/distributed/client.py:3164: UserWarning: Sending large graph of size 123.51 MiB.\nThis may cause some slowdown.\nConsider scattering data ahead of time and using futures.\n  warnings.warn(\n2024-06-02 21:24:17,892 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle 03b81160c0e3780cffda58fc1995cc55 initialized by task ('shuffle-transfer-03b81160c0e3780cffda58fc1995cc55', 9) executed on worker tcp://127.0.0.1:42839\n2024-06-02 21:24:19,446 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle 03b81160c0e3780cffda58fc1995cc55 deactivated due to stimulus 'task-finished-1717363459.4442878'\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value counts took: 2.5369818210601807 seconds\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local_disk0/.ephemeral_nfs/envs/pythonEnv-e5fa1c31-e351-4056-a9dc-f3f4a9c54411/lib/python3.9/site-packages/distributed/client.py:3164: UserWarning: Sending large graph of size 370.48 MiB.\nThis may cause some slowdown.\nConsider scattering data ahead of time and using futures.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "groupby statistics took: 4.098268747329712 seconds\nOut[29]: 4.098268747329712"
     ]
    }
   ],
   "source": [
    "#benchmark(read_file_parquet, df=None, benchmarks=dask_benchmarks, name='read file')\n",
    "benchmark(count, df=dask_data, benchmarks=dask_benchmarks, name='count')\n",
    "benchmark(count_index_length, df=dask_data, benchmarks=dask_benchmarks, name='count index length')\n",
    "benchmark(mean, df=dask_data, benchmarks=dask_benchmarks, name='mean')\n",
    "benchmark(standard_deviation, df=dask_data, benchmarks=dask_benchmarks, name='standard deviation')\n",
    "benchmark(mean_of_sum, df=dask_data, benchmarks=dask_benchmarks, name='mean of columns addition')\n",
    "benchmark(sum_columns, df=dask_data, benchmarks=dask_benchmarks, name='addition of columns')\n",
    "benchmark(mean_of_product, df=dask_data, benchmarks=dask_benchmarks, name='mean of columns multiplication')\n",
    "benchmark(product_columns, df=dask_data, benchmarks=dask_benchmarks, name='multiplication of columns')\n",
    "benchmark(value_counts, df=dask_data, benchmarks=dask_benchmarks, name='value counts')\n",
    "# No column for this\n",
    "# benchmark(mean_of_complicated_arithmetic_operation, df=dask_data, benchmarks=dask_benchmarks, name='mean of complex arithmetic ops')\n",
    "# benchmark(complicated_arithmetic_operation, df=dask_data, benchmarks=dask_benchmarks, name='complex arithmetic ops')\n",
    "benchmark(groupby_statistics, df=dask_data, benchmarks=dask_benchmarks, name='groupby statistics')\n",
    "# benchmark(join_count, dask_data, benchmarks=dask_benchmarks, name='join count', other=other)\n",
    "# benchmark(join_data, dask_data, benchmarks=dask_benchmarks, name='join', other=other) # cant join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a6234d3-bc2e-4cd4-b849-35d88b2af205",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Operations with filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff7755f9-8696-4a68-b484-6b8f5885cb11",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "expr_filter = (dask_data.tip_amount >= 1) & (dask_data.tip_amount <= 5)\n",
    "\n",
    "def filter_data(df):\n",
    "    return df[expr_filter]\n",
    "  \n",
    "dask_filtered = filter_data(dask_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fae27585-6477-4209-a813-367cb06ac13f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local_disk0/.ephemeral_nfs/envs/pythonEnv-e5fa1c31-e351-4056-a9dc-f3f4a9c54411/lib/python3.9/site-packages/distributed/client.py:3164: UserWarning: Sending large graph of size 2.10 GiB.\nThis may cause some slowdown.\nConsider scattering data ahead of time and using futures.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered count took: 44.74642515182495 seconds\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local_disk0/.ephemeral_nfs/envs/pythonEnv-e5fa1c31-e351-4056-a9dc-f3f4a9c54411/lib/python3.9/site-packages/distributed/client.py:3164: UserWarning: Sending large graph of size 2.10 GiB.\nThis may cause some slowdown.\nConsider scattering data ahead of time and using futures.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered count index length took: 50.05057716369629 seconds\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local_disk0/.ephemeral_nfs/envs/pythonEnv-e5fa1c31-e351-4056-a9dc-f3f4a9c54411/lib/python3.9/site-packages/distributed/client.py:3164: UserWarning: Sending large graph of size 246.99 MiB.\nThis may cause some slowdown.\nConsider scattering data ahead of time and using futures.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered mean took: 3.3873913288116455 seconds\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local_disk0/.ephemeral_nfs/envs/pythonEnv-e5fa1c31-e351-4056-a9dc-f3f4a9c54411/lib/python3.9/site-packages/distributed/client.py:3164: UserWarning: Sending large graph of size 246.99 MiB.\nThis may cause some slowdown.\nConsider scattering data ahead of time and using futures.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered standard deviation took: 2.5518338680267334 seconds\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local_disk0/.ephemeral_nfs/envs/pythonEnv-e5fa1c31-e351-4056-a9dc-f3f4a9c54411/lib/python3.9/site-packages/distributed/client.py:3164: UserWarning: Sending large graph of size 246.99 MiB.\nThis may cause some slowdown.\nConsider scattering data ahead of time and using futures.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered mean of columns addition took: 2.100897789001465 seconds\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local_disk0/.ephemeral_nfs/envs/pythonEnv-e5fa1c31-e351-4056-a9dc-f3f4a9c54411/lib/python3.9/site-packages/distributed/client.py:3164: UserWarning: Sending large graph of size 246.99 MiB.\nThis may cause some slowdown.\nConsider scattering data ahead of time and using futures.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered addition of columns took: 4.536018371582031 seconds\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local_disk0/.ephemeral_nfs/envs/pythonEnv-e5fa1c31-e351-4056-a9dc-f3f4a9c54411/lib/python3.9/site-packages/distributed/client.py:3164: UserWarning: Sending large graph of size 246.99 MiB.\nThis may cause some slowdown.\nConsider scattering data ahead of time and using futures.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered mean of columns multiplication took: 2.053715705871582 seconds\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local_disk0/.ephemeral_nfs/envs/pythonEnv-e5fa1c31-e351-4056-a9dc-f3f4a9c54411/lib/python3.9/site-packages/distributed/client.py:3164: UserWarning: Sending large graph of size 246.99 MiB.\nThis may cause some slowdown.\nConsider scattering data ahead of time and using futures.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered multiplication of columns took: 3.2464606761932373 seconds\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local_disk0/.ephemeral_nfs/envs/pythonEnv-e5fa1c31-e351-4056-a9dc-f3f4a9c54411/lib/python3.9/site-packages/distributed/client.py:3164: UserWarning: Sending large graph of size 247.00 MiB.\nThis may cause some slowdown.\nConsider scattering data ahead of time and using futures.\n  warnings.warn(\n2024-06-02 21:27:50,277 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle 5922a0f479a5046dfed9945ab7329233 initialized by task ('shuffle-transfer-5922a0f479a5046dfed9945ab7329233', 9) executed on worker tcp://127.0.0.1:42839\n2024-06-02 21:27:51,888 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle 5922a0f479a5046dfed9945ab7329233 deactivated due to stimulus 'task-finished-1717363671.8853662'\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered value counts took: 3.4203221797943115 seconds\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local_disk0/.ephemeral_nfs/envs/pythonEnv-e5fa1c31-e351-4056-a9dc-f3f4a9c54411/lib/python3.9/site-packages/distributed/client.py:3164: UserWarning: Sending large graph of size 370.49 MiB.\nThis may cause some slowdown.\nConsider scattering data ahead of time and using futures.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered groupby statistics took: 4.089606046676636 seconds\nOut[31]: 4.089606046676636"
     ]
    }
   ],
   "source": [
    "benchmark(count, dask_filtered, benchmarks=dask_benchmarks, name='filtered count')\n",
    "benchmark(count_index_length, dask_filtered, benchmarks=dask_benchmarks, name='filtered count index length')\n",
    "benchmark(mean, dask_filtered, benchmarks=dask_benchmarks, name='filtered mean')\n",
    "benchmark(standard_deviation, dask_filtered, benchmarks=dask_benchmarks, name='filtered standard deviation')\n",
    "benchmark(mean_of_sum, dask_filtered, benchmarks=dask_benchmarks, name ='filtered mean of columns addition')\n",
    "benchmark(sum_columns, df=dask_filtered, benchmarks=dask_benchmarks, name='filtered addition of columns')\n",
    "benchmark(mean_of_product, dask_filtered, benchmarks=dask_benchmarks, name ='filtered mean of columns multiplication')\n",
    "benchmark(product_columns, df=dask_filtered, benchmarks=dask_benchmarks, name='filtered multiplication of columns')\n",
    "#benchmark(mean_of_complicated_arithmetic_operation, dask_filtered, benchmarks=dask_benchmarks, name='filtered mean of complex arithmetic ops')\n",
    "#benchmark(complicated_arithmetic_operation, dask_filtered, benchmarks=dask_benchmarks, name='filtered complex arithmetic ops')\n",
    "benchmark(value_counts, dask_filtered, benchmarks=dask_benchmarks, name ='filtered value counts')\n",
    "benchmark(groupby_statistics, dask_filtered, benchmarks=dask_benchmarks, name='filtered groupby statistics')\n",
    "\n",
    "# other = groupby_statistics(dask_filtered)\n",
    "# other.columns = pd.Index([e[0]+'_' + e[1] for e in other.columns.tolist()])\n",
    "\n",
    "# benchmark(join_count, dask_filtered, benchmarks=dask_benchmarks, name='filtered join count', other=other)\n",
    "# benchmark(join_data, dask_filtered, benchmarks=dask_benchmarks, name='filtered join', other=other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42f0a631-26a7-4672-a615-d372854f4bb6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duration</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>task</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>0.058250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>count index length</th>\n",
       "      <td>36.148871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.394890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>standard deviation</th>\n",
       "      <td>1.553624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean of columns addition</th>\n",
       "      <td>2.125428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>addition of columns</th>\n",
       "      <td>3.785042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean of columns multiplication</th>\n",
       "      <td>1.795114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>multiplication of columns</th>\n",
       "      <td>3.949759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>value counts</th>\n",
       "      <td>2.536982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>groupby statistics</th>\n",
       "      <td>4.098269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>filtered count</th>\n",
       "      <td>44.746425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>filtered count index length</th>\n",
       "      <td>50.050577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>filtered mean</th>\n",
       "      <td>3.387391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>filtered standard deviation</th>\n",
       "      <td>2.551834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>filtered mean of columns addition</th>\n",
       "      <td>2.100898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>filtered addition of columns</th>\n",
       "      <td>4.536018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>filtered mean of columns multiplication</th>\n",
       "      <td>2.053716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>filtered multiplication of columns</th>\n",
       "      <td>3.246461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>filtered value counts</th>\n",
       "      <td>3.420322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>filtered groupby statistics</th>\n",
       "      <td>4.089606</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>duration</th>\n    </tr>\n    <tr>\n      <th>task</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>0.058250</td>\n    </tr>\n    <tr>\n      <th>count index length</th>\n      <td>36.148871</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>2.394890</td>\n    </tr>\n    <tr>\n      <th>standard deviation</th>\n      <td>1.553624</td>\n    </tr>\n    <tr>\n      <th>mean of columns addition</th>\n      <td>2.125428</td>\n    </tr>\n    <tr>\n      <th>addition of columns</th>\n      <td>3.785042</td>\n    </tr>\n    <tr>\n      <th>mean of columns multiplication</th>\n      <td>1.795114</td>\n    </tr>\n    <tr>\n      <th>multiplication of columns</th>\n      <td>3.949759</td>\n    </tr>\n    <tr>\n      <th>value counts</th>\n      <td>2.536982</td>\n    </tr>\n    <tr>\n      <th>groupby statistics</th>\n      <td>4.098269</td>\n    </tr>\n    <tr>\n      <th>filtered count</th>\n      <td>44.746425</td>\n    </tr>\n    <tr>\n      <th>filtered count index length</th>\n      <td>50.050577</td>\n    </tr>\n    <tr>\n      <th>filtered mean</th>\n      <td>3.387391</td>\n    </tr>\n    <tr>\n      <th>filtered standard deviation</th>\n      <td>2.551834</td>\n    </tr>\n    <tr>\n      <th>filtered mean of columns addition</th>\n      <td>2.100898</td>\n    </tr>\n    <tr>\n      <th>filtered addition of columns</th>\n      <td>4.536018</td>\n    </tr>\n    <tr>\n      <th>filtered mean of columns multiplication</th>\n      <td>2.053716</td>\n    </tr>\n    <tr>\n      <th>filtered multiplication of columns</th>\n      <td>3.246461</td>\n    </tr>\n    <tr>\n      <th>filtered value counts</th>\n      <td>3.420322</td>\n    </tr>\n    <tr>\n      <th>filtered groupby statistics</th>\n      <td>4.089606</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dask_res_temp = get_results(dask_benchmarks).set_index('task')\n",
    "dask_res_temp"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Dask",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
